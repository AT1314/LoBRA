vault_path: "./vault"
collection_name: "local_brain"
qdrant_url: "http://localhost:6333"

# models (via Ollama) - Optimized for 24GB RAM (M4 Pro)
# With 24GB RAM, you can use larger, more capable models
# Recommended: llama3.1:8b for best balance of quality and speed
# Alternative: mistral:7b, qwen2.5:7b, or even llama3.1:70b if you want maximum quality
chat_model: "llama3.1:8b"  # More capable model, ~5-6GB RAM usage
embed_model: "nomic-embed-text"

# chunking (optimized for 24GB RAM)
chunk_size: 800      # Larger chunks for better context understanding
chunk_overlap: 150   # Better overlap for context continuity

# retrieval (comprehensive for better results)
top_k_vector: 8      # More semantic matches
top_k_bm25: 8        # More keyword matches
fusion_k: 10         # More comprehensive final results

# metadata fields you care about in front-matter
frontmatter_keys: ["title","date","tags","course","project","links","summary"]

