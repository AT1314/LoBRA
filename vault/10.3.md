---
title: "10.3"
date: "2025-10-02"
source: "inbox/10.3.pdf"
format: "PDF Document"
pages: "1"
converted: "2025-11-11 14:57:20"
---

# 10.3

**Pages:** 1


## Page 1

10.3
Summary:
PipeDream is introduced to add inter-batch pipelining to intra-batch parallelism to 
better overlap computation with communication and reduce the communication 
whenever possible. To deal with bi-directional training, PipeDream uses a 1F1B 
schedule with weight versioning (“stashingˮ) to maintain correct gradients while 
overlapping forward/backward passes, and it auto-partitions layers across 
workers to balance load and minimize cross-stage traffic. Across tasks like image, 
translation, language modeling, and video tasks, PipeDream trains to the same 
accuracy up to 5.3 faster than commonly used intra-batch parallelism 
techniques.
Questions:
 By making the pipeline more smooth (less pipeline bubbles), what tradeoff 
does PipeDream make? i.e., in what aspect is GPipe better than PipeDream?
PipeDream uses weight stashing to improve the hardware utilization. It causes 
staleness and memory overhead compared to GPipe. Also, we need to introduce 
Vertical Sync to eliminate the potential inconsistencies across stages. Conversely, 
GPipe keeps weights synchronous and consistent per mini-batch, bringing more 
reliable accuracy at the cost of more pipeline bubbles and lower throughput.
 What type of parallelism do you think is most widely adopted in practice? 
Why?
Hybrid intra-batch parallelism, using both model parallelism and data parallelism. 
The paper explicitly says the default in practice is intra-batch parallelization: “The 
most common way to train DNN models today is intra-batch parallelization, 
where a single iteration of training is split across available workers,ˮ then 
immediately breaks this down into data, model, and hybrid forms.
10 . 3
1