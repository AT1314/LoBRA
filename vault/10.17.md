---
title: "10.17"
date: "2025-10-15"
source: "inbox/10.17.pdf"
format: "PDF Document"
pages: "1"
converted: "2025-11-11 14:57:20"
---

# 10.17

**Pages:** 1


## Page 1

10.17
Summary:
Preble is a distributed LLM serving system that exploits shared prompt prefixes 
across requests: it introduces E2 Exploitation  Exploration) scheduling to place 
requests on GPUs that already cache the longest matching prefix when reuse 
saves more work than rebalancing, and otherwise load-balances with a prompt-
aware cost model. A hierarchical scheduler plus autoscaling/replication of hot 
prefixes keeps utilization high and fairness intact. In studied workloads, prompts 
are 372494 longer than outputs and 8597% of prompt tokens are shared, so 
accelerating prefill via KV-reuse dominates. Preble cuts average latency by 1.5
14.5 and p99 latency by 210 versus vLLM/SGLang on multi-GPU clusters.
Questions:
 What will happen if all requests are scheduled only based on where their 
matched prefix cache reside?
Some GPUs may hold those popular prefixes and cause poor fairness of GPU 
utilization. Tail latency balloons.
 What does Preble do when a lot of requests share the same prefix (i.e., highly 
skewed workloads)?
Preble uses E2 scheduling Exploit  Explore): it e xploit s  prefix locality when it 
reduces work enough, but e xplor es  alternative placements when a cache-holding 
GPU is saturated to keep the cluster balanced. It replicates hot prefixes across 
more GPUs (autoscaling the replication factor) so matching requests can be 
spread, and uses a prompt-aware cost model to decide when to route to a 
cached copy versus load-balance elsewhere. Together, this limits hotspots, 
preserves cache benefits for the hot prefix, and keeps p50/p99 latency low under 
skew.
10 . 17
1