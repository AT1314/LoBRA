---
title: "10.10"
date: "2025-10-09"
source: "inbox/10.10.pdf"
format: "PDF Document"
pages: "1"
converted: "2025-11-11 14:57:20"
---

# 10.10

**Pages:** 1


## Page 1

10.10
Summary:
The paper proposes PagedAttention inspired by OS virtual memory and paging 
techniques to store each requestʼs KV cache in fixed-size “pagesˮ, to avoid 
fragmentation and can share KV cache within and across requests, dramatically 
improving batchability. Built on top of this, vLLM, 
an end-to-end LLM serving system, achieves near-zero KV-cache waste and 
serves 24x throughput, outperforming the previous state-of-theart solutions such 
as FasterTransformer and Orca, while supporting LLMs like GPT, OPT, LLaMA with 
various sizes.
Questions:
 List at least two reasons why GPU memory for KV cache is wasted without 
PagedAttention.
First, internal/external fragmentation causes a lot of wastes. Second, there are 
some duplicates in KV cache wasting the GPU memory.
 What are the tradeoffs of using larger/smaller block sizes in PagedAttention?
Smaller blocks → finer granularity: less internal fragmentation and easier KV 
sharing/migration, but more metadata and lookup overhead and potentially 
lower kernel efficiency.
Larger blocks → fewer lookups and better kernel locality/utilization, but more 
internal waste and less flexibility when sequences diverge in length; you also 
reduce how precisely you can share or reclaim KV memory.
10 . 10
1