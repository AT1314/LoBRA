---
title: "11.7"
date: "2025-11-06"
source: "inbox/11.7.pdf"
format: "PDF Document"
pages: "2"
converted: "2025-11-11 14:57:20"
---

# 11.7

**Pages:** 2


## Page 1

11.7
Summary:
MemGPT proposes treating an LLM like an operating system that manages 
a virtual context: it “pagesˮ information between a small working window and 
larger external stores (short-term, long-term, and documents) so conversations 
and tasks can exceed the native context length. It also introduces interrupts and 
tool-style functions so the model can proactively read/write memory and control 
flow. The paper shows this OS-inspired memory management helps with large-
document analysis and multi-session chat where ordinary context windows 
break down. 
Questions:
 What's the memory-hierarchy architecture proposed in MemGPT? What are 
the tradeoffs this architecture introduces in terms of latency?
Working context LLM window): the “RAMˮ where the current turn + a tiny 
subset of facts live. Fastest, but tiny—forces frequent paging.
Short-term memory STM/inbox/scratch): model-writable notes/summaries it 
can pull into the window. Moderate latency (serialize/deserialize + prompt 
tokens).
Long-term memory LTM / vector store): embeddings over past chats/files; 
fetched via search + summarization before “paging inˮ. Higher 
latency (embed/query/IO but huge capacity.
Cold storage / documents: raw files, databases, web. Highest 
latency (disk/network + parse/summarize), used sparingly.
Trade-offs: Fewer prompt tokens (cheaper) but extra interrupts/tool calls to 
read/write memory; embedding + retrieval + summarization add variable 
delay; poor heuristics can thrash (too many small fetches) or starve (fetch too 
late).
 How would you redesign the memory hierachy for throughput-oriented tasks?
11. 7
1

## Page 2

Batch & cache memory ops: micro-batch embedding/retrieval across 
requests; add a shared hot-key cache and a small in-RAM ANN tier before the 
full index.
Use coarser “pagesˮ with precompression: fetch larger, structured 
summaries IDs/tuples) less often; prefetch by task type/tool hints to avoid 
frequent interrupts.
Avoid on-the-fly work: precompute embeddings for known corpora; apply 
small delta updates instead of full re-summaries.
Pipeline wisely: cap memory ops per step and overlap retrieval with 
generation (read-through/write-behind) to smooth tail latency.
11. 7
2