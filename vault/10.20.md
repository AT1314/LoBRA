---
title: "10.20"
date: "2025-10-19"
source: "inbox/10.20.pdf"
format: "PDF Document"
pages: "1"
converted: "2025-11-11 14:57:20"
---

# 10.20

**Pages:** 1


## Page 1

10.21
Summary:
DistServe proposes disaggregating LLM prefill and decoding onto different GPUs 
to remove interference between the two phases and to meet dual SLOs—TTFT for 
prefill and TPOT for decoding—more efficiently. Given TTFT/TPOT targets, it co-
optimizes resource allocation and parallelism per phase, and places phases to 
respect cluster bandwidth, maximizing per-GPU goodput (requests/sec within 
SLO. On popular LLMs and workloads, DistServe serves up to 7.4 more 
requests or achieves 12.6 tighter SLOs than state-of-the-art colocated designs 
while satisfying 90% of requestsʼ latency constraints.
Questions:
 How does DistServe designed their system to mitigate the overhead of KV 
cache migration?
Co-locates paired prefill/decoding stages on the same node to keep KV moves on 
NVLink; uses pull-based, buffered transfers and async copy/NCCL to overlap 
comms with compute—making KV transfer a tiny slice of latency.
 Do you think DistServe can scale to a very large cluster, say 100/1k/10k GPUs 
for serving? Why or why not?
100 GPUs:
yes—design maps well with bandwidth-aware placement.
1k GPUs:
maybe, but the network and central controller become bottlenecks unless you 
add higher-bandwidth fabrics and hierarchical control
10k GPUs:
not without major changes (fully distributed control, stronger fault isolation, and 
fabric-aware routing).
10 .21
1