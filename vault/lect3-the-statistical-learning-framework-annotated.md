---
title: "lect3-the-statistical-learning-framework-annotated"
date: "2025-01-16"
source: "inbox/lect3-the-statistical-learning-framework-annotated.pdf"
format: "PDF Document"
pages: "61"
converted: "2025-10-31 10:24:35"
---

# lect3-the-statistical-learning-framework-annotated

**Pages:** 61


## Page 1

The Statistical Learning 
Framework
CSE 251A Winter 2022
Instructor: Taylor Berg-Kirkpatrick
Based on notes by Sanjoy Dasgupta & Kamalika Chaudhuri

## Page 2

The Underlying Distribution
â€¢ The big question: in what situations does training 
data help us develop a model that will work well on 
future data?

## Page 3

The Underlying Distribution
â€¢ The big question: in what situations does training 
data help us develop a model that will work well on 
future data?
â€¢ What is the link between the two?

## Page 4

The Underlying Distribution
â€¢ The big question: in what situations does training 
data help us develop a model that will work well on 
future data?
â€¢ What is the link between the two?
â€¢ One such setting:  when the statistical learning 
assumption holds.

## Page 5

The Underlying Distribution
â€¢ The big question: in what situations does training 
data help us develop a model that will work well on 
future data?
â€¢ What is the link between the two?
â€¢ One such setting:  when the statistical learning 
assumption holds.
â€¢Assumption: All data (past, present, and future) is 
drawn i.i.d. (independent and identically distributed) 
from some (unknown) underlying distribution on 
.ğ’³ Ã— ğ’´

## Page 6

The Underlying Distribution
â€¢ Call this distribution .P

## Page 7

The Underlying Distribution
â€¢ Call this distribution .P
â€¢e.g. Want to predict whether ER waiting room 
patients have the ï¬‚y, based on age and temperature.

## Page 8

The Underlying Distribution
â€¢ Call this distribution .P
â€¢e.g. Want to predict whether ER waiting room 
patients have the ï¬‚y, based on age and temperature.
: (age, temp)        x ğ’³ = {0,â€¦ ,100} Ã— {97,98,â€¦ ,105}

## Page 9

The Underlying Distribution
â€¢ Call this distribution .P
â€¢e.g. Want to predict whether ER waiting room 
patients have the ï¬‚y, based on age and temperature.
: (age, temp)        x ğ’³ = {0,â€¦ ,100} Ã— {97,98,â€¦ ,105}
: 0 (no ï¬‚u) or 1 (ï¬‚u)            y ğ’´ = {0,1}

## Page 10

The Underlying Distribution
â€¢ Call this distribution .P
â€¢e.g. Want to predict whether ER waiting room 
patients have the ï¬‚y, based on age and temperature.
: (age, temp)        x ğ’³ = {0,â€¦ ,100} Ã— {97,98,â€¦ ,105}
: 0 (no ï¬‚u) or 1 (ï¬‚u)            y ğ’´ = {0,1}
As we get more training data, we 
get a better idea of .P
age
temp âˆ’
++
+
+++
+
+++
âˆ’âˆ’âˆ’
âˆ’
âˆ’
âˆ’
âˆ’ âˆ’âˆ’
âˆ’
âˆ’

## Page 11

The Underlying Distribution
â€¢ Three ways to sample from P

## Page 12

The Underlying Distribution
â€¢ Three ways to sample from P
1. Draw (x, y) âˆ¼ P

## Page 13

The Underlying Distribution
â€¢ Three ways to sample from P
1. Draw (x, y) âˆ¼ P
2. Draw  according to its marginal distribution.         
Then draw  according to the conditional distribution 
of .
y
x
x| y

## Page 14

The Underlying Distribution
â€¢ Three ways to sample from P
1. Draw (x, y) âˆ¼ P
2. Draw  according to its marginal distribution.         
Then draw  according to the conditional distribution 
of .
y
x
x| y
3. Draw  according to its marginal distribution.       
Then draw  according to the conditional distribution 
of .
x
y
y| x

## Page 15

The Underlying Distribution
â€¢ Deï¬ne:
 distribution on Î¼ : ğ’³
 conditional distribution Î· : y| x

## Page 16

The Underlying Distribution
â€¢ Deï¬ne:
 distribution on Î¼ : ğ’³
 conditional distribution Î· : y| x
â€¢ Example:
ğ’´ = {0,1}
age
temp
105
97
0 100
ğ’³ =

## Page 17

The Underlying Distribution
â€¢ Deï¬ne:
 distribution on Î¼ : ğ’³
 conditional distribution Î· : y| x
â€¢ Example:
Maybe:   : uniform distribution on   Î¼ ğ’³ Î¼(x) = 1
900 âˆ€x âˆˆ ğ’³
ğ’´ = {0,1}
age
temp
105
97
0 100
ğ’³ =

## Page 18

The Underlying Distribution
â€¢ Deï¬ne:
 distribution on Î¼ : ğ’³
 conditional distribution Î· : y| x
â€¢ Example:
Maybe:   : uniform distribution on   Î¼ ğ’³ Î¼(x) = 1
900 âˆ€x âˆˆ ğ’³
             Î·(x) = Pr(y = 1| x) âˆˆ [0,1]
ğ’´ = {0,1}
age
temp
105
97
0 100
ğ’³ =

## Page 19

The Underlying Distribution
â€¢ Deï¬ne:
 distribution on Î¼ : ğ’³
 conditional distribution Î· : y| x
â€¢ Example:
Maybe:   : uniform distribution on   Î¼ ğ’³ Î¼(x) = 1
900 âˆ€x âˆˆ ğ’³
             Î·(x) = Pr(y = 1| x) âˆˆ [0,1]
â€¢Why isn't  either 0 or 1?Î·(x)
ğ’´ = {0,1}
age
temp
105
97
0 100
ğ’³ =

## Page 20

The Underlying Distribution
â€¢ Deï¬ne:
 distribution on Î¼ : ğ’³
 conditional distribution Î· : y| x
â€¢ Example:
Maybe:   : uniform distribution on   Î¼ ğ’³ Î¼(x) = 1
900 âˆ€x âˆˆ ğ’³
             Î·(x) = Pr(y = 1| x) âˆˆ [0,1]
â€¢Why isn't  either 0 or 1?Î·(x)
â€¢Sometimes it is -- e.g. MNIST digit classiï¬cation
ğ’´ = {0,1}
age
temp
105
97
0 100
ğ’³ =

## Page 21

The Underlying Distribution
â€¢ Deï¬ne:
 distribution on Î¼ : ğ’³
 conditional distribution Î· : y| x
â€¢ Example:
Maybe:   : uniform distribution on   Î¼ ğ’³ Î¼(x) = 1
900 âˆ€x âˆˆ ğ’³
             Î·(x) = Pr(y = 1| x) âˆˆ [0,1]
â€¢Why isn't  either 0 or 1?Î·(x)
â€¢Sometimes it is -- e.g. MNIST digit classiï¬cation
â€¢In other cases, there is inherent uncertainty.
ğ’´ = {0,1}
age
temp
105
97
0 100
ğ’³ =

## Page 22

Risk
â€¢ Classiï¬er:   h : ğ’³ â†’ ğ’´

## Page 23

Risk
â€¢ Classiï¬er:   h : ğ’³ â†’ ğ’´
â€¢ Risk:    R(h) = Pr(x,y)âˆ¼P(h(x) â‰  y)

## Page 24

Risk
â€¢ Classiï¬er:   h : ğ’³ â†’ ğ’´
â€¢ Risk:    R(h) = Pr(x,y)âˆ¼P(h(x) â‰  y)
â€¢ Bayes-optimal classiï¬er:    with smallest 
possible risk
h* : ğ’³ â†’ ğ’´

## Page 25

Risk
â€¢ Classiï¬er:   h : ğ’³ â†’ ğ’´
â€¢ Risk:    R(h) = Pr(x,y)âˆ¼P(h(x) â‰  y)
â€¢ Bayes-optimal classiï¬er:    with smallest 
possible risk
h* : ğ’³ â†’ ğ’´
When ,   ğ’´ = {0,1} h*(x) = {
1 if Î·(x) â‰¥ 1
2
0 otherwise

## Page 26

Risk
â€¢ Classiï¬er:   h : ğ’³ â†’ ğ’´
â€¢ Risk:    R(h) = Pr(x,y)âˆ¼P(h(x) â‰  y)
â€¢ Bayes-optimal classiï¬er:    with smallest 
possible risk
h* : ğ’³ â†’ ğ’´
When ,   ğ’´ = {0,1} h*(x) = {
1 if Î·(x) â‰¥ 1
2
0 otherwise
Bayes' risk:    R* = R(h*) = ğ”¼[ min (Î·(x),1 âˆ’ Î·(x))]

## Page 27

Consistency
â€¢ Let's say we have a learning algorithm that returns 
classiï¬er  after seeing  training pointshn n

## Page 28

Consistency
â€¢ Let's say we have a learning algorithm that returns 
classiï¬er  after seeing  training pointshn n
â€¢ The algorithm is consistent iff  as R(hn) â†’ R* n â†’ âˆ

## Page 29

Consistency
â€¢ Let's say we have a learning algorithm that returns 
classiï¬er  after seeing  training pointshn n
â€¢ The algorithm is consistent iff  as R(hn) â†’ R* n â†’ âˆ
â€¢ Example: Nearest neighbor classiï¬cation

## Page 30

Consistency
â€¢ Let's say we have a learning algorithm that returns 
classiï¬er  after seeing  training pointshn n
â€¢ The algorithm is consistent iff  as R(hn) â†’ R* n â†’ âˆ
â€¢ Example: Nearest neighbor classiï¬cation
â€¢ Pick  points at random from n P = (Î¼, Î·)

## Page 31

Consistency
â€¢ Let's say we have a learning algorithm that returns 
classiï¬er  after seeing  training pointshn n
â€¢ The algorithm is consistent iff  as R(hn) â†’ R* n â†’ âˆ
â€¢ Example: Nearest neighbor classiï¬cation
â€¢ Pick  points at random from n P = (Î¼, Î·)
â€¢ Let  be the 1-NN classiï¬er trained on this datahn

## Page 32

Consistency
â€¢ Let's say we have a learning algorithm that returns 
classiï¬er  after seeing  training pointshn n
â€¢ The algorithm is consistent iff  as R(hn) â†’ R* n â†’ âˆ
â€¢ Example: Nearest neighbor classiï¬cation
â€¢ Pick  points at random from n P = (Î¼, Î·)
â€¢ Let  be the 1-NN classiï¬er trained on this datahn
â€¢ Is  consistent?hn

## Page 33

Consistency
â€¢ Example: Nearest neighbor classiï¬cation

## Page 34

Consistency
â€¢ Example: Nearest neighbor classiï¬cation
â€¢ Is  consistent?hn

## Page 35

Consistency
â€¢ Example: Nearest neighbor classiï¬cation
â€¢ Is  consistent?hn
No! e.g. consider the follow distribution:

## Page 36

Consistency
â€¢ Example: Nearest neighbor classiï¬cation
â€¢ Is  consistent?hn
No! e.g. consider the follow distribution:
, ,  is uniform on ,  everywhereğ’³ = [0,1] ğ’´ = {0,1} Î¼ ğ’³ Î· = 1
4

## Page 37

Consistency
â€¢ Example: Nearest neighbor classiï¬cation
â€¢ Is  consistent?hn
No! e.g. consider the follow distribution:
, ,  is uniform on ,  everywhereğ’³ = [0,1] ğ’´ = {0,1} Î¼ ğ’³ Î· = 1
4
Bayes' optimal classiï¬er:     ,    Risk h* = 0 R* = 1
4

## Page 38

Consistency
â€¢ Example: Nearest neighbor classiï¬cation
â€¢ Is  consistent?hn
No! e.g. consider the follow distribution:
, ,  is uniform on ,  everywhereğ’³ = [0,1] ğ’´ = {0,1} Î¼ ğ’³ Î· = 1
4
Bayes' optimal classiï¬er:     ,    Risk h* = 0 R* = 1
4
1-NN classiï¬er:  error two coins of bias 1/4 disagree  Pr( ) = Pr( )

## Page 39

Consistency
â€¢ Example: Nearest neighbor classiï¬cation
â€¢ Is  consistent?hn
No! e.g. consider the follow distribution:
, ,  is uniform on ,  everywhereğ’³ = [0,1] ğ’´ = {0,1} Î¼ ğ’³ Î· = 1
4
Bayes' optimal classiï¬er:     ,    Risk h* = 0 R* = 1
4
1-NN classiï¬er:  error two coins of bias 1/4 disagree  Pr( ) = Pr( )
= 2 â‹… 3
4 â‹… 1
4 = 3
8 > R*

## Page 40

Consistency
â€¢ Example: Nearest neighbor classiï¬cation
â€¢ Is  consistent?hn
No! e.g. consider the follow distribution:
, ,  is uniform on ,  everywhereğ’³ = [0,1] ğ’´ = {0,1} Î¼ ğ’³ Î· = 1
4
Bayes' optimal classiï¬er:     ,    Risk h* = 0 R* = 1
4
1-NN classiï¬er:  error two coins of bias 1/4 disagree  Pr( ) = Pr( )
= 2 â‹… 3
4 â‹… 1
4 = 3
8 > R*
In fact, can show , at most twice Bayes' 
risk.
R(hn) â†’ 2R*(1 âˆ’ R*)

## Page 41

Consistency of k-NN
â€¢ But k-NN is consistent if  grows with k n

## Page 42

Consistency of k-NN
â€¢ But k-NN is consistent if  grows with k n
â€¢ Theorem:

## Page 43

Consistency of k-NN
â€¢ But k-NN is consistent if  grows with k n
â€¢ Theorem:
â€¢ Let  be a metric space.ğ’³

## Page 44

Consistency of k-NN
â€¢ But k-NN is consistent if  grows with k n
â€¢ Theorem:
â€¢ Let  be a metric space.ğ’³
â€¢ Let  be the k-NN classiï¬er based on  training pointshn,k n âˆ¼ P

## Page 45

Consistency of k-NN
â€¢ But k-NN is consistent if  grows with k n
â€¢ Theorem:
â€¢ Let  be a metric space.ğ’³
â€¢ Let  be the k-NN classiï¬er based on  training pointshn,k n âˆ¼ P
â€¢ Suppose:

## Page 46

Consistency of k-NN
â€¢ But k-NN is consistent if  grows with k n
â€¢ Theorem:
â€¢ Let  be a metric space.ğ’³
â€¢ Let  be the k-NN classiï¬er based on  training pointshn,k n âˆ¼ P
â€¢ Suppose:
â€¢  is a growing function of  with k n k â†’ âˆ, k
n â†’ 0

## Page 47

Consistency of k-NN
â€¢ But k-NN is consistent if  grows with k n
â€¢ Theorem:
â€¢ Let  be a metric space.ğ’³
â€¢ Let  be the k-NN classiï¬er based on  training pointshn,k n âˆ¼ P
â€¢ Suppose:
â€¢  is a growing function of  with k n k â†’ âˆ, k
n â†’ 0
â€¢  is continuousÎ·

## Page 48

Consistency of k-NN
â€¢ But k-NN is consistent if  grows with k n
â€¢ Theorem:
â€¢ Let  be a metric space.ğ’³
â€¢ Let  be the k-NN classiï¬er based on  training pointshn,k n âˆ¼ P
â€¢ Suppose:
â€¢  is a growing function of  with k n k â†’ âˆ, k
n â†’ 0
â€¢  is continuousÎ·
â€¢ Then: 

## Page 49

Consistency of k-NN
â€¢ But k-NN is consistent if  grows with k n
â€¢ Theorem:
â€¢ Let  be a metric space.ğ’³
â€¢ Let  be the k-NN classiï¬er based on  training pointshn,k n âˆ¼ P
â€¢ Suppose:
â€¢  is a growing function of  with k n k â†’ âˆ, k
n â†’ 0
â€¢  is continuousÎ·
â€¢ Then: 
â€¢ R(hn,k) â†’ R*

## Page 50

Consistency of k-NN
â€¢ Proof sketch:  Pick any point .  We'll show that x âˆˆ ğ’³
hn,k(x) â†’ h*(x)

## Page 51

Consistency of k-NN
â€¢ Proof sketch:  Pick any point .  We'll show that x âˆˆ ğ’³
hn,k(x) â†’ h*(x)
(i) If , it doesn't matter what we predict. So, 
without loss of generality, let 
Î·(x) = 1
2
Î·(x) < 1
2

## Page 52

Consistency of k-NN
â€¢ Proof sketch:  Pick any point .  We'll show that x âˆˆ ğ’³
hn,k(x) â†’ h*(x)
(i) If , it doesn't matter what we predict. So, 
without loss of generality, let 
Î·(x) = 1
2
Î·(x) < 1
2
(ii) By continuity,  in some ball  around . Î· < 1
2 B x
B
x

## Page 53

Consistency of k-NN
â€¢ Proof sketch:  Pick any point .  We'll show that x âˆˆ ğ’³
hn,k(x) â†’ h*(x)
(i) If , it doesn't matter what we predict. So, 
without loss of generality, let 
Î·(x) = 1
2
Î·(x) < 1
2
(ii) By continuity,  in some ball  around . Î· < 1
2 B x
random point falls in BPr( ) = Î¼(B)
B
x

## Page 54

Consistency of k-NN
â€¢ Proof sketch:  Pick any point .  We'll show that x âˆˆ ğ’³
hn,k(x) â†’ h*(x)
(i) If , it doesn't matter what we predict. So, 
without loss of generality, let 
Î·(x) = 1
2
Î·(x) < 1
2
(ii) By continuity,  in some ball  around . Î· < 1
2 B x
random point falls in BPr( ) = Î¼(B)
(iii) As :n â†’ âˆ
B
x

## Page 55

Consistency of k-NN
â€¢ Proof sketch:  Pick any point .  We'll show that x âˆˆ ğ’³
hn,k(x) â†’ h*(x)
(i) If , it doesn't matter what we predict. So, 
without loss of generality, let 
Î·(x) = 1
2
Î·(x) < 1
2
(ii) By continuity,  in some ball  around . Î· < 1
2 B x
random point falls in BPr( ) = Î¼(B)
(iii) As :n â†’ âˆ
â€¢ k-NN of  fall in Pr( x B) â†’ 1
B
x

## Page 56

Consistency of k-NN
â€¢ Proof sketch:  Pick any point .  We'll show that x âˆˆ ğ’³
hn,k(x) â†’ h*(x)
(i) If , it doesn't matter what we predict. So, 
without loss of generality, let 
Î·(x) = 1
2
Î·(x) < 1
2
(ii) By continuity,  in some ball  around . Î· < 1
2 B x
random point falls in BPr( ) = Î¼(B)
(iii) As :n â†’ âˆ
â€¢ k-NN of  fall in Pr( x B) â†’ 1
â€¢ majority vote of their labels is 0Pr( ) â†’ 1
B
x

## Page 57

Consistency of k-NN 
â€¢ Interesting question:
â€¢ How does k-NN behave on a non-metric space? 

## Page 58

Limitations of the Framework
â€¢ Central assumption:  All data is i.i.d. from a ï¬xed, 
unknown, distribution over .ğ’³ Ã— ğ’´
â€¢ When might this not hold?

## Page 59

Limitations of the Framework
â€¢ Central assumption:  All data is i.i.d. from a ï¬xed, 
unknown, distribution over .ğ’³ Ã— ğ’´
â€¢ When might this not hold?
â€¢ Shifting distribution

## Page 60

Limitations of the Framework
â€¢ Central assumption:  All data is i.i.d. from a ï¬xed, 
unknown, distribution over .ğ’³ Ã— ğ’´
â€¢ When might this not hold?
â€¢ Shifting distribution
(i)  changing,  ï¬xed:  e.g. different handwriting / speech 
distributions
Î¼ Î·

## Page 61

Limitations of the Framework
â€¢ Central assumption:  All data is i.i.d. from a ï¬xed, 
unknown, distribution over .ğ’³ Ã— ğ’´
â€¢ When might this not hold?
â€¢ Shifting distribution
(i)  changing,  ï¬xed:  e.g. different handwriting / speech 
distributions
Î¼ Î·
(ii)  both change:    e.g. document categorization 
(politics, sports, etc.)
Î¼, Î·