---
title: "11.5"
date: "2025-11-04"
source: "inbox/11.5.pdf"
format: "PDF Document"
pages: "1"
converted: "2025-11-11 14:57:20"
---

# 11.5

**Pages:** 1


## Page 1

11.5
Summary:
HybridFlow proposes a flexible RLHF training framework that hybridizes single-
controller and multi-controller paradigms: a hierarchical programming model 
expresses RLHF as a dataflow, while execution decouples intra-node compute 
from inter-node transfers to keep control overhead low. It introduces a 3D 
HybridEngine and auto-placement to schedule actor/critic/reward/reference 
models efficiently across clusters, yielding higher throughput and better resource 
use than prior RLHF systems; the open-source implementation is VERL. 
Questions:
 How is the LLM inference (rollout) phase in RL training different from LLM 
serving (for end users)? List at least two differences.
Metrics & shape: Serving optimizes latency/TTFT/TPOT per request; rollouts 
optimize throughput of tokens + trajectories/sec and cheap multi-sample 
generation per prompt (often N samples per state).
Outputs required: Serving returns text; rollouts also need per-token logprobs, 
sometimes value-head logits, and randomness control (temperature/seeds) 
for on-policy sampling.
 Can you think of a way to improve the LLM inference phase in RL training?
KV-sharing multi-sample decode: For each prompt/state, prefill once, then 
generate M parallel continuations that reuse the same KV cache 
PagedAttention/continuous batching). Add speculative decoding with a 
lightweight “draftˮ head (or prefix-retrieval proposals) so the target policy 
advances multiple tokens per verify step, and overlap rollout → reward/value → 
advantage computation in an actor–learner pipeline. This cuts prefill duplication, 
improves GPU utilization on decode, and hides RM/critic time behind ongoing 
generation.
11. 5
1