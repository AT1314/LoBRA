---
title: "10.1"
date: "2025-09-30"
source: "inbox/10.1.pdf"
format: "PDF Document"
pages: "2"
converted: "2025-11-11 14:57:20"
---

# 10.1

**Pages:** 2


## Page 1

10.1
Summary:
This passage first explains LLM text generation in simple words, and token-based 
metrics arenʼt comparable across different models. Then it discusses user-
perceived latency hinges on Time-to-First-Token TTFT and Time-Per-Output-
Token TPOT, with overall latency  TTFT  TPOT  output length; throughput 
rises with batching but can slow per-request speed, and decode is often memory-
bandwidth-bound, captured by utilization metrics like MBU.Batching boosts 
overall throughput but can slow individual responses, and the decode step is often 
limited by memory bandwidth rather than raw compute.
Questions:
1. What are the key performance metrics to look out for in LLM inference? 
Time to First Token TTFT how long before anything appears.
Time per Output Token TPOT the steady “typing speedˮ once tokens start 
flowing.
End-to-end latency: total wait time  TTFT  TPOT  output length (long 
answers dominate).
Throughput: total tokens per second across users; batching helps system-
wide but may hurt per-request snappiness.
Memory bandwidth efficiency (e.g., MBU how close you are to saturating 
the hardwareʼs memory bandwidth during decode.
Some important things:  output length drives latency more than input; bigger 
models donʼt give linear latency gains; and batching/serving strategy shapes the 
latency–throughput trade-off.
2. What would be the important metrics to evaluate if you were to run a language 
model on your PC/tablet?
10 . 1
1

## Page 2

TTFT and sustained TPOT do responses feel instant, and can you keep 515 
tok/s for your usual tasks?
Fits-in-memory check: do the weights  KV cache fit in VRAM at your 
target context length, without paging?
Efficiency on your chip: is your TPOT close to what your CPU/GPU memory 
bandwidth should allow?
Concurrency impact: what happens to latency if you open a second chat or 
run a tool alongside?
Quantization trade-offs: 4-bit/8-bit often makes local runs possible; confirm 
the quality holds for your prompts.
Battery and heat: tokens per battery percent, throttling, and fan noise—
especially on thin-and-light devices.
10 . 1
2