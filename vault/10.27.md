---
title: "10.27"
date: "2025-10-26"
source: "inbox/10.27.pdf"
format: "PDF Document"
pages: "1"
converted: "2025-11-11 14:57:20"
---

# 10.27

**Pages:** 1


## Page 1

1 0 . 2 7 
Summary:
InferCept is a serving system for LLM agents that call tools, wait for users, or 
pause mid-task. Today, most systems treat every pause as if the request ended 
and later restart from scratch, which means they throw away the KV cache and 
waste a lot of GPU time redoing work. It fixes that by treating pauses as 
“interceptions,ˮ and for each paused request it decides: keep the KV cache on 
GPU, swap it to CPU, or drop it and recompute later. It also overlaps 
swapping/recompute with other work so GPUs stay busy. The result is the server 
can handle about 1.62 more agent requests at the same latency.
Questions:
 What are the three ways of dealing with KV cache when a model calls an API?
Disc ar d:  treat the tool call as the end of the request, drop its KV cache, and 
later recompute it from scratch.
Pr eser v e:  keep the KV cache in GPU memory during the tool call so you can 
resume instantly.
Sw ap:  move the KV cache from GPU to CPU at the tool call and bring it back 
later.
 Why does InferCept consider running requests that are not currently calling 
tools when calculating GPU memory waste?
InferCept includes other (non-intercepted) running requests in the GPU memory 
waste calculation because actions like recomputing dropped KV or swapping KV 
in/out slow down the whole iteration, which forces those other requests to just sit 
in memory without making forward progress. That “stall time,ˮ multiplied by how 
much memory those other requests are holding, is also wasted memory capacity 
that could have served new work, so it must be counted to choose the globally 
best action.
10 .27
1