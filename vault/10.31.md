---
title: "10.31"
date: "2025-10-30"
source: "inbox/10.31.pdf"
format: "PDF Document"
pages: "2"
converted: "2025-11-11 14:57:20"
---

# 10.31

**Pages:** 2


## Page 1

10.31
Summary:
DeepSeek-R1 introduces a family of “reasoning-firstˮ LLMs trained primarily with 
reinforcement learning GRPO, including R1Zero RL without SFT and R1 (adds 
a small cold-start dataset). The core claim is that RL alone can elicit strong, 
emergent reasoning behaviors and sizable gains on math/code/reasoning 
benchmarks versus base models, positioning R1 as competitive with 
contemporary “o-seriesˮ style models.
Demystifying Delays in Reasoning presents the first temporal performance 
study of deep-research agents (o3-deep-research, GPT5, LangChain-DR on 
DeepResearchBench, instrumenting runs to attribute latency and token costs. Key 
findings: web search dominates end-to-end latency (often 5073%, up to 91%, 
while final answer generation consumes most tokens due to lengthy retrieved 
context—implying tool latency and retrieval design are the main levers to speed up 
real-world reasoning workflows. 
Questions:
 What are the main advantages and risks of using this purely RL-based 
approach (without supervised fine-tuning) to incentivize chain-of-thought 
reasoning in LLMs?
Pros
Can unlock emergent reasoning behaviors without being constrained by 
SFTʼs human-pattern biases; R1Zero shows strong reasoning emerging from 
RL alone.
Avoids large CoT labeling costs; achieves competitive reasoning 
performance after RL R1. 
Risks
10 . 31
1

## Page 2

Instability & odd outputs R1Zero exhibits poor readability, repetition, and 
language mixing before additional stabilization. 
Reward-hacking/specification risk and heavy compute: outcomes depend on 
proxy rewards and large-scale RL infrastructure.
 What implications does the study results have for designing efficient large 
reasoning systems? Any proposal for improving request latency?
The study finds web search dominates end-to-end latency, while final 
answer generation burns most tokens due to long retrieved context—so tool 
I/O and retrieval design are the main levers.
Practical moves: bound and parallelize searches, cache results across 
steps/sessions, and prefetch likely URLs; compress context via aggressive 
reranking/summarization before generation to trim token usage. 
Pipeline tweaks: enforce step budgets for search, stream intermediate 
findings, and route “no-searchˮ queries to a fast path; these target the 
measured hotspots (tool latency and bloated context) directly. 
10 . 31
2