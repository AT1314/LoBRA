---
title: "11.12"
date: "2025-11-11"
source: "inbox/11.12.pdf"
format: "PDF Document"
pages: "2"
converted: "2025-11-11 14:57:20"
---

# 11.12

**Pages:** 2


## Page 1

11.12
Summary:
METIS SOSP ʼ25 introduces a RAG serving system that jointly (i) adapts per-
query RAG configurations (e.g., how many chunks to retrieve, whether to 
summarize first, synthesis method) and (ii) schedules queries to fit GPU memory 
and batching, explicitly trading off quality vs. delay. It first uses a lightweight 
LLM profiler to prune each queryʼs configuration space, then picks the best 
remaining config together with a placement/batching decision—delivering 1.6
2.8 lower latency and 1.84.5 higher throughput than vLLM, Parrot, and 
AdaptiveRAG at matched or better answer quality.
Questions:
 How does METIS's LLM profiler's data inform the adaptation mechanism's 
scheduling decision at query time?
It predicts, per request, a few config options with estimated quality, latency, 
token/memory cost, and batchability.
It keeps only the Pareto-good options.
The scheduler then picks one option + a placement/batch that fits current 
GPU load while meeting the quality/latency target; if the queue spikes, it 
downshifts to a cheaper option from that set.
 Consider an agent that must perform both RAG and tool use. How could the 
core principles of METIS be extended to create a "quality-aware scheduler" 
for this type of agents?
Treat each request as a small plan with tunables: RAG (k, reranker, 
summarize), tools (which/parallelism/timeouts), LLM decode 
(model/temperature).
Build a light profiler to predict quality + cost for a few candidate plans, keep 
the Pareto set, and schedule plan + placement jointly.
11. 12
1

## Page 2

Under load, degrade gracefully (fewer docs, cached results, summary-first) 
while enforcing a minimum quality; cache embeddings/tool outputs to speed 
repeats and cut p99.
11. 12
2