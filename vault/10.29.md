---
title: "10.29"
date: "2025-10-28"
source: "inbox/10.29.pdf"
format: "PDF Document"
pages: "2"
converted: "2025-11-11 14:57:20"
---

# 10.29

**Pages:** 2


## Page 1

10.29
Summary:
MegaScale-Infer is a system for serving very large Mixture-of-Experts MoE 
language models faster and cheaper by splitting each layer into two parts — 
attention and experts FFNs) — and running those parts on different GPUs. This 
lets the system scale and batch them separately: attention can be replicated on 
memory-friendly GPUs, while FFN “expertsˮ can be packed and batched on 
compute-efficient GPUs, keeping both sides busy and fixing the usual low-
utilization problem in MoE serving. It also uses a ping-pong pipeline and a custom 
communication layer so the two sides can pass tokens back and forth efficiently, 
which gives up to 2 higher throughput per GPU (and better throughput per 
dollar on mixed GPU clusters).
Questions:
 Why can the disaggregation of attention and FFN improve the GPU utilization 
during MoE serving?
Disaggregating lets you run attention and FFN on different GPU pools and scale 
them differently. In MoE, each token only activates a few experts, so a single 
expert FFN often sees a tiny slice of the batch and sits mostly idle; by separating 
attention from FFN, MegaScale-Infer can replicate attention nodes, merge tokens 
from many attention replicas, and feed much larger combined batches into each 
expert FFN, which drives FFN GPUs to high utilization again. It also allows you to 
place attention on memory/bandwidth-optimized GPUs (for KV cache access) and 
FFNs on compute-optimized GPUs, so each part runs on hardware it can actually 
saturate.
 Does the ping-pong pipeline impose any inherent constraints on the model 
architecture? If yes, what problems will arise when such constraints are not 
satisfied?
Yes. The ping-pong pipeline slices a batch into microbatches and “bouncesˮ them 
between attention GPUs and FFN GPUs; this only works well if attention and FFN 
have roughly similar per-microbatch compute time so that while one side is 
10 .29
1

## Page 2

working, the other side has the next microbatch ready, and communication can be 
hidden. If one side is much slower (for example, FFN time per microbatch is far 
longer than attention time), the faster side goes idle waiting, pipeline bubbles 
appear, you stop hiding communication, and total throughput and GPU utilization 
drop.
10 .29
2