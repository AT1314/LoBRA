---
title: "10.6"
date: "2025-10-04"
source: "inbox/10.6.pdf"
format: "PDF Document"
pages: "2"
converted: "2025-11-11 14:57:20"
---

# 10.6

**Pages:** 2


## Page 1

10.6
Summary:
HLAT reports training two decoder-only LLMs 7B and 70B entirely on AWS 
Trainium using up to 4096 accelerators across 256 trn1.32xlarge instances, 
over 1.8T tokens, and shows their quality is comparable to similarly sized 
LLaMA/OpenLLaMA models trained on GPUs/TPUs. The paperʼs key contribution 
is demonstrating a full, stable Trainium pre-training stack (via NeuronX 
Distributed Training) plus practical techniques—online dataloader, layer 
coalescing, selective activation checkpointing, precision strategy choices, and 
robust fault recovery—to reach competitive accuracy and cost/performance. They 
open-source training scripts/configs and provide best practices, arguing Trainium 
offers roughly GPU-class compute at lower price while achieving steady 
convergence and benchmark parity on reasoning, knowledge, math, and coding 
suites. 
Questions:
 How does HLAT determine its parallelism strategies and system optimizations, 
and how might these trade-offs evolve as model size or cluster scale 
increases?
HLAT determines its parallelism strategies mainly based on memory fit and 
communication balance, using 3D parallelism via NxDT and picks different 
TP/PP/DP for 7B vs 70B. The authors also enable sequence parallelism and 
ZeRO1 to reduce activation/optimizer memory.
For system optimizations, throughput & stability optimizations are chosen 
empirically for Trainium, including techniques like online dataloader, layer 
coalesing, selective activation checkpointing, precision strategy, and fault 
recovery mechanisms.
As model grows, Data pipeline becomes communication-bounded. We need to 
exploit TP/PP but at the cost of more inter-stage traffic, pipeline bubbles, and 
kernel fragmentation—hence the need for layer coalescing and checkpointing. 
10 . 6
1

## Page 2

As clusters grow, the data path and fault recovery dominate. HLAT explicitly 
optimizes the dataloader and uses automatic checkpoint/restart to keep large runs 
alive. Expect to dial up PP/TP and checkpointing, down SR-style stochastic 
rounding, and rely more on mixed precision and memory-saving tricks to keep 
convergence stable at scale.
 Which stage or component of pre-training do you consider most critical in 
practice, and why?
The most critical is training stability policy - precision and optimizer. 
Why: HLAT shows that BF16 with stochastic rounding can be unstable for 70B. 
They switch to a mixed-precision recipe to stabilize convergence. Without a 
stable precision/optimizer setup, bigger models waste compute or diverge—no 
amount of parallelism tuning can save a run that doesnʼt converge.
The data pipeline also matters.
10 . 6
2