---
title: "10.13"
date: "2025-10-10"
source: "inbox/10.13.pdf"
format: "PDF Document"
pages: "1"
converted: "2025-11-11 14:57:20"
---

# 10.13

**Pages:** 1


## Page 1

10.13
Summary:
The paper proposes speculative decoding, where we uses a small “draftˮ model 
to speculatively propose several next tokens, then run the large target model in 
parallel to verify/accept as many as possible with a rejection-correction rule that 
preserves the exact output distribution of the target model. This cuts the number 
of serial target-model steps and yields 23 wall-clock speedups on T5XXL 
without retraining or changing outputs, analysis formalizes acceptance rates and 
shows how speedup depends on the draft–target match. The method generalizes 
classic speculative execution to stochastic sampling, needs only an auxiliary 
faster model, and is especially effective when inference is 
memory/communication-bound and extra compute can be overlapped.
Questions:
 List two drawbacks of speculative decoding
Speedup is fragile to acceptance rate. If the mismatch happens a lot, it will 
cause more compute for drafting plus verification/rollback, and can end up 
slower than plain decoding.
Higher system complexity and resource overhead.
 In an agentic setting where LLMs are repeatedly called, can you think of a way 
to speculate output tokens without using a draft model Mq in the paper)?
Nearest-neighbor continuation reuse: Retrieve the most similar prior prompt from 
a vector index and copy its next-N tokens as a speculative proposal; the target 
verifies, correcting at the first mismatch.
10 . 13
1