---
title: "10.22"
date: "2025-10-21"
source: "inbox/10.22.pdf"
format: "PDF Document"
pages: "1"
converted: "2025-11-11 14:57:20"
---

# 10.22

**Pages:** 1


## Page 1

1 0 . 2 2 
Summar y: 
DeepSpeed-Ulysses introduces a sequence-parallel training method that 
partitions input along the sequence dimension and uses two all-to-all collectives 
around attention so each GPU sees the full sequence for a subset of heads, then 
re-partitions, keeping communication constant as sequence length and device 
count grow proportionally. Compared to prior sequence-parallel approaches (e.g., 
Megatron-SP, ColAISP, Ulysses achieves 10 lower communication volume, 
supports dense/sparse/FlashAttention, composes with ZeRO3, and trains 4 
longer sequences with up to 2.5 higher throughput (sustaining 54% of peak per 
GPU. The design targets long-context LLMs (up to million-token sequences) with 
minimal code changes and is orthogonal to data/tensor/pipeline parallelism.
Questions: 
 Can DeepSpeed-Ulysses work when the number of GPUs is higher than the 
number of heads? Why or why not?
No. Ulysses redistributes Q/K/V so that each GPU owns the full sequence but only 
a disjoint subset of attention heads, so the parallelism degree is upper-bounded 
by the number of heads; with GQA/MQA (few KV heads) this constraint bites even 
harder. In such cases youʼd need to combine Ulysses with other parallelism (e.g., 
DP/PP or replicate heads, but pure Ulysses canʼt scale past the head count.
 What is the latency impact for one transformer layer caused by DeepSpeed-
Ulysses?
Ulysses introduces two all-to-all collectives per layer: one to gather QKV before 
attention and another to return the output context afterward. The per-link traffic is 
3Nh  Nh)/P  4Nh/P (sequence length N, hidden size h, P GPUs), so the latency 
cost is essentially these two all-to-alls (amortized by 1/P plus minimal local 
overhead—significantly less per-link volume than Megatronʼs SP, which moves 
4Nh per link.
10 .22
1