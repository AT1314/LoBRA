---
title: "11.3"
date: "2025-11-02"
source: "inbox/11.3.pdf"
format: "PDF Document"
pages: "2"
converted: "2025-11-11 14:57:20"
---

# 11.3

**Pages:** 2


## Page 1

11.3
Summary:
Hugging Face Agents Course – “What are Tools?ˮ.
Defines a tool as a function the LLM can call to act (e.g., web search, retrieval, API 
calls, image gen) and explains that good tools have precise names, descriptions, 
typed inputs/outputs, and are injected via the system prompt. The page shows 
how to auto-describe tools from Python (e.g., via a decorator) and notes MCP can 
standardize tool interfaces across frameworks.
Anthropic  Model Context Protocol MCP.
MCP is an open standard for secure, two-way connections between AI apps and 
external data/tools; developers either expose data via MCP servers or build MCP
client apps. The launch includes a spec+SDKs, Claude Desktop support, and 
open-source server examples (e.g., Google Drive, Slack, GitHub), aiming to 
replace bespoke connectors with a single, reusable protocol.
Questions:
 For tool-based AI systems, do you believe we should have more autonomy 
(more agent like) or less autonomy (more workflow like)? Find a use case to 
justify your answer.
It depends. But by default it should be: “less autonomyˮ for high-stakes, “more 
autonomyˮ for repetitive operations.
High-stakes (prefer workflow-like): e.g., medical prior-auth submission: 
strict steps, compliance checks, audit trails. A workflow with human approvals 
reduces risk, keeps provenance, and is easier to certify.
Repetitive ops (prefer agent-like): e.g., daily e-commerce price scraping + 
report: let the agent pick tools, handle captchas/rate limits, and self-heal 
minor failures—humans only review anomalies.
11. 3
1

## Page 2

Rule of thumb: start constrained (workflow), gradually grant autonomy where 
metrics show low error/low blast radius; add guardrails (schemas, typed tools, 
quotas, canary runs).
 Can you think of some challenges of benchmarking and ensuring robustness 
in agents that depend on external tools?
Non-deterministic tools: web content, API latency, and UI changes make runs 
irreproducible; need cached snapshots, versioned fixtures, and “time-boxedˮ 
replays.
Evaluation leakage: an agent can “look up the answerˮ rather than reason; 
benchmarks must separate retrieval success from policy quality (e.g., ablate 
internet vs. closed-book).
End-to-end metrics ambiguity: success may be binary while paths differ; 
require step-level logs, cost/latency, and side-effect checks (e.g., file actually 
created, DB mutation validated).
11. 3
2